{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rantanj15/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rantanj15/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "lanc_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', sep='\\t', header=None)\n",
    "df.iloc[0]\n",
    "corpus = df.values.tolist()\n",
    "corpus[0]\n",
    "corpus = [item for sublist in corpus for item in sublist]\n",
    "corpus[0]\n",
    "labels = []\n",
    "\n",
    "#Remove id and class values from the beginning and add labels to a list\n",
    "for i in range(0,len(corpus)):\n",
    "    ind = corpus[i].index(\"#\")\n",
    "    corpus[i] = corpus[i][ind+1:]\n",
    "    labels.append(int(corpus[i][0])-1)\n",
    "    ind = corpus[i].index(\"#\")\n",
    "    corpus[i] = corpus[i][ind+1:]\n",
    "    \n",
    "def lanc_stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [lanc_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'aft', 'al', 'ar', 'becaus', 'bef', 'doe', 'dur', 'furth', 'hav', 'mor', 'ont', 'oth', 'ourselv', 'ov', 'sam', 'som', 'themselv', 'ther', 'thes', 'thi', 'thos', 'und', 'wer', 'wher', 'whil', 'wil', 'yo', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "#tfidf for KMeans\n",
    "vec = TfidfVectorizer(stop_words=stopwords.words('english'), \n",
    "                      tokenizer=lanc_stemming_tokenizer,\n",
    "                      use_idf=True,\n",
    "                      norm='l2') \n",
    "\n",
    "matrix = vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7208134122527954"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Single KMeans model with seed for reproducibility\n",
    "kmeans_model = KMeans(n_clusters=5, n_init=1, random_state=420)\n",
    "kmeans_predictions = kmeans_model.fit_predict(matrix)\n",
    "\n",
    "normalized_mutual_info_score(labels, kmeans_predictions, average_method='geometric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per kmeans cluster:\n",
      "Cluster 0:\n",
      " compil\n",
      " program\n",
      " cod\n",
      "Cluster 1:\n",
      " robot\n",
      " control\n",
      " system\n",
      "Cluster 2:\n",
      " sec\n",
      " encrypt\n",
      " cryptograph\n",
      "Cluster 3:\n",
      " im\n",
      " detect\n",
      " vis\n",
      "Cluster 4:\n",
      " databas\n",
      " dat\n",
      " rel\n"
     ]
    }
   ],
   "source": [
    "#Keywords for the KMeans clustering\n",
    "print(\"Top terms per kmeans cluster:\")\n",
    "order_centroids = kmeans_model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vec.get_feature_names()\n",
    "for i in range(5):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :3]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7018217077564124"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean score for 100 KMeans clusterings with random seeds\n",
    "scores = []\n",
    "\n",
    "for i in range(0,100):\n",
    "    kmeans_model = KMeans(n_clusters=5)\n",
    "    kmeans_predictions = kmeans_model.fit_predict(matrix)\n",
    "    \n",
    "    score = normalized_mutual_info_score(labels, kmeans_predictions, average_method='geometric')\n",
    "    scores.append(score)\n",
    "\n",
    "np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abl', 'abov', 'accord', 'ad', 'adopt', 'ads', 'affect', 'aft', 'afterward', 'ain', 'al', 'alon', 'alongsid', 'alway', 'annount', 'anoth', 'anym', 'anyon', 'anyth', 'anywh', 'app', 'apprecy', 'appropry', 'approxim', 'ar', 'asid', 'assocy', 'au', 'avail', 'aw', 'becam', 'becaus', 'becom', 'bef', 'believ', 'besid', 'bet', 'bil', 'cal', 'cam', 'capt', 'caus', 'chang', 'clear', 'comput', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'cur', 'dar', 'dat', 'definit', 'describ', 'despit', 'didn', 'diff', 'direct', 'doe', 'doesn', 'don', 'downward', 'dur', 'eith', 'elev', 'els', 'elsewh', 'entir', 'espec', 'ev', 'everm', 'everyon', 'everyth', 'everywh', 'exact', 'exampl', 'exceiv', 'fair', 'farth', 'fif', 'fil', 'fir', 'fiv', 'follow', 'forev', 'form', 'ful', 'furth', 'furtherm', 'gav', 'giv', 'goe', 'gon', 'greet', 'hadn', 'hap', 'hard', 'hasn', 'hav', 'hent', 'hereaft', 'hims', 'hith', 'hom', 'hop', 'howev', 'hundr', 'ign', 'immedy', 'import', 'ind', 'indee', 'inform', 'insid', 'insof', 'int', 'inv', 'isn', 'key', 'larg', 'lat', 'lik', 'likew', 'lin', 'littl', 'll', 'mad', 'main', 'mak', 'mayb', 'mayn', 'meantim', 'meanwhil', 'mer', 'mightn', 'mil', 'min', 'mon', 'mor', 'moreov', 'mov', 'mustn', 'mys', 'nam', 'necess', 'nee', 'needn', 'neith', 'nev', 'nin', 'noon', 'norm', 'noth', 'notwithstand', 'nowh', 'obvy', 'oft', 'omit', 'ont', 'opposit', 'oth', 'otherw', 'oughtn', 'ourselv', 'outsid', 'ov', 'overal', 'ow', 'pag', 'particul', 'perhap', 'plac', 'pleas', 'plu', 'poor', 'poss', 'pot', 'predomin', 'pres', 'presum', 'prevy', 'prim', 'prob', 'prompt', 'provid', 'quick', 'quit', 'rath', 'ready', 'real', 'reason', 'rec', 'regard', 'rel', 'respect', 'result', 'sam', 'sect', 'selv', 'sens', 'sery', 'sev', 'shal', 'shan', 'shouldn', 'sid', 'sign', 'simil', 'sint', 'slight', 'som', 'someon', 'someth', 'sometim', 'somewh', 'spec', 'stat', 'stil', 'subst', 'success', 'sufficy', 'sur', 'tak', 'tel', 'tend', 'themselv', 'thent', 'ther', 'thereaft', 'theref', 'thes', 'theyr', 'thi', 'thos', 'thu', 'togeth', 'tri', 'tru', 'twelv', 'twic', 'und', 'undernea', 'undo', 'unfortun', 'unlik', 'upward', 'valu', 'vary', 've', 'vers', 'wasn', 'wel', 'welcom', 'wer', 'whatev', 'whenev', 'whent', 'wher', 'wherea', 'whereaft', 'wherev', 'wheth', 'whichev', 'whil', 'whith', 'whoev', 'whol', 'whomev', 'wid', 'wil', 'won', 'wond', 'word', 'wouldn', 'ye', 'yo', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "#Different stop word list for spectral clustering and also different tfidf calculation and LSA (truncatedSVD)\n",
    "stop_words_long = [\"able\",\"about\",\"above\",\"abroad\",\"according\",\"accordingly\",\"across\",\"actually\",\"adj\",\"after\",\"afterwards\",\"again\",\"against\",\"ago\",\"ahead\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"alongside\",\"already\",\"also\",\"although\",\"always\",\"am\",\"amid\",\"amidst\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"a's\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"back\",\"backward\",\"backwards\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"begin\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"came\",\"can\",\"cannot\",\"cant\",\"can't\",\"caption\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"c'mon\",\"co\",\"co.\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"c's\",\"currently\",\"dare\",\"daren't\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"directly\",\"do\",\"does\",\"doesn't\",\"doing\",\"done\",\"don't\",\"down\",\"downwards\",\"during\",\"each\",\"edu\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"evermore\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"fairly\",\"far\",\"farther\",\"few\",\"fewer\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"forever\",\"former\",\"formerly\",\"forth\",\"forward\",\"found\",\"four\",\"from\",\"further\",\"furthermore\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"had\",\"hadn't\",\"half\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he'd\",\"he'll\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"hereafter\",\"hereby\",\"herein\",\"here's\",\"hereupon\",\"hers\",\"herself\",\"he's\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"hundred\",\"i'd\",\"ie\",\"if\",\"ignored\",\"i'll\",\"i'm\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"inc.\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"inside\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"its\",\"it's\",\"itself\",\"i've\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"likewise\",\"little\",\"look\",\"looking\",\"looks\",\"low\",\"lower\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mayn't\",\"me\",\"mean\",\"meantime\",\"meanwhile\",\"merely\",\"might\",\"mightn't\",\"mine\",\"minus\",\"miss\",\"more\",\"moreover\",\"most\",\"mostly\",\"mr\",\"mrs\",\"much\",\"must\",\"mustn't\",\"my\",\"myself\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needn't\",\"needs\",\"neither\",\"never\",\"neverf\",\"neverless\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"no\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"no-one\",\"nor\",\"normally\",\"not\",\"nothing\",\"notwithstanding\",\"novel\",\"now\",\"nowhere\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"one's\",\"only\",\"onto\",\"opposite\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"oughtn't\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provided\",\"provides\",\"que\",\"quite\",\"qv\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"recent\",\"recently\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"round\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"someday\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that'll\",\"thats\",\"that's\",\"that've\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"thereafter\",\"thereby\",\"there'd\",\"therefore\",\"therein\",\"there'll\",\"there're\",\"theres\",\"there's\",\"thereupon\",\"there've\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"thing\",\"things\",\"think\",\"third\",\"thirty\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"till\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"t's\",\"twice\",\"two\",\"un\",\"under\",\"underneath\",\"undoing\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"upwards\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"versus\",\"very\",\"via\",\"viz\",\"vs\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"welcome\",\"well\",\"we'll\",\"went\",\"were\",\"we're\",\"weren't\",\"we've\",\"what\",\"whatever\",\"what'll\",\"what's\",\"what've\",\"when\",\"whence\",\"whenever\",\"where\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"where's\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"whichever\",\"while\",\"whilst\",\"whither\",\"who\",\"who'd\",\"whoever\",\"whole\",\"who'll\",\"whom\",\"whomever\",\"who's\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"wonder\",\"won't\",\"would\",\"wouldn't\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"your\",\"you're\",\"yours\",\"yourself\",\"yourselves\",\"you've\",\"zero\",\"a\",\"how's\",\"i\",\"when's\",\"why's\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"j\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"uucp\",\"w\",\"x\",\"y\",\"z\",\"I\",\"www\",\"amount\",\"bill\",\"bottom\",\"call\",\"computer\",\"con\",\"couldnt\",\"cry\",\"de\",\"describe\",\"detail\",\"due\",\"eleven\",\"empty\",\"fifteen\",\"fifty\",\"fill\",\"find\",\"fire\",\"forty\",\"front\",\"full\",\"give\",\"hasnt\",\"herse\",\"himse\",\"interest\",\"itse”\",\"mill\",\"move\",\"myse”\",\"part\",\"put\",\"show\",\"side\",\"sincere\",\"sixty\",\"system\",\"ten\",\"thick\",\"thin\",\"top\",\"twelve\",\"twenty\",\"abst\",\"accordance\",\"act\",\"added\",\"adopted\",\"affected\",\"affecting\",\"affects\",\"ah\",\"announce\",\"anymore\",\"apparently\",\"approximately\",\"aren\",\"arent\",\"arise\",\"auth\",\"beginning\",\"beginnings\",\"begins\",\"biol\",\"briefly\",\"ca\",\"date\",\"ed\",\"effect\",\"et-al\",\"ff\",\"fix\",\"gave\",\"giving\",\"heres\",\"hes\",\"hid\",\"home\",\"id\",\"im\",\"immediately\",\"importance\",\"important\",\"index\",\"information\",\"invention\",\"itd\",\"keys\",\"kg\",\"km\",\"largely\",\"lets\",\"line\",\"'ll\",\"means\",\"mg\",\"million\",\"ml\",\"mug\",\"na\",\"nay\",\"necessarily\",\"nos\",\"noted\",\"obtain\",\"obtained\",\"omitted\",\"ord\",\"owing\",\"page\",\"pages\",\"poorly\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"promptly\",\"proud\",\"quickly\",\"ran\",\"readily\",\"ref\",\"refs\",\"related\",\"research\",\"resulted\",\"resulting\",\"results\",\"run\",\"sec\",\"section\",\"shed\",\"shes\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"slightly\",\"somethan\",\"specifically\",\"state\",\"states\",\"stop\",\"strongly\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"thered\",\"thereof\",\"therere\",\"thereto\",\"theyd\",\"theyre\",\"thou\",\"thoughh\",\"thousand\",\"throug\",\"til\",\"tip\",\"ts\",\"ups\",\"usefully\",\"usefulness\",\"'ve\",\"vol\",\"vols\",\"wed\",\"whats\",\"wheres\",\"whim\",\"whod\",\"whos\",\"widely\",\"words\",\"world\",\"youd\",\"youre\"]\n",
    "\n",
    "vec = TfidfVectorizer(stop_words=stop_words_long, \n",
    "                      tokenizer=lanc_stemming_tokenizer,\n",
    "                      use_idf=True,\n",
    "                      norm='l2',\n",
    "                      ngram_range=(1,3)) \n",
    "\n",
    "matrix = vec.fit_transform(corpus)\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "matrix = svd.fit_transform(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8135081307404697"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Single Spectral clustering model with seed for reproducibility\n",
    "spectral_model = SpectralClustering(n_clusters=5, assign_labels='discretize', random_state=420)\n",
    "spectral_predictions = spectral_model.fit_predict(matrix)\n",
    "\n",
    "normalized_mutual_info_score(labels, spectral_predictions, average_method='geometric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8135081307404698"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean score for 100 Spectral clusterings with random seeds\n",
    "scores = []\n",
    "\n",
    "for i in range(0,100):\n",
    "    spectral_model = SpectralClustering(n_clusters=5, assign_labels='discretize')\n",
    "    spectral_predictions = spectral_model.fit_predict(matrix)\n",
    "    \n",
    "    score = normalized_mutual_info_score(labels, spectral_predictions, average_method='geometric')\n",
    "    scores.append(score)\n",
    "\n",
    "np.mean(scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
